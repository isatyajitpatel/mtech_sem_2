{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Recommendation System for Purchase Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Prepare the enviroments with libraries and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "\n",
    "#Data\n",
    "import sqlalchemy as sql\n",
    "\n",
    "#Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor, BaselineOnly\n",
    "from surprise import KNNBasic,KNNWithMeans,KNNWithZScore,KNNBaseline\n",
    "from surprise import SVD,SVDpp,NMF\n",
    "from surprise import SlopeOne, CoClustering\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.accuracy import rmse, mae\n",
    "from surprise import dump\n",
    "\n",
    "#Model Tracking\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import glob\n",
    "import configparser\n",
    "from collections import defaultdict\n",
    "import tempfile\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment Variables\n",
    "dbconnPath = './dbconn.properties'\n",
    "interdata = '../data/interim/'\n",
    "outdata = '../data/processed/'\n",
    "outmodels = '../models'\n",
    "outreports = '../reports/'\n",
    "\n",
    "# Set dbconnection variables\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(dbconnPath)\n",
    "params = config\n",
    "db_host=params.get('CONN', 'host')\n",
    "db_port=params.get('CONN', 'port')\n",
    "db_user=params.get('CONN', 'user')\n",
    "db_pwd=params.get('CONN', 'password')\n",
    "db_name=params.get('CONN', 'database')\n",
    "\n",
    "# Set connection string\n",
    "connection_str = f'mysql+pymysql://{db_user}:{db_pwd}@{db_host}:{db_port}/{db_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We have to:\n",
    "\n",
    "1) Split each list of items in the products column into rows\n",
    "\n",
    "2) Count the number of products bought by a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "engine = sql.create_engine(connection_str)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{db_name} contains {engine.table_names()[0]}, {engine.table_names()[1]} tables')\n",
    "# print(f'{db_name} contains {engine.table_names()[0]} table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tables in dataframes (Don't do that if you have a large dataset. Use just samples)\n",
    "# df_cus = pd.read_sql(\"select * from CUSTOMERID\", connection)\n",
    "df_trx = pd.read_sql(\"select * from TRANSACTIONS\", connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_cus.shape)\n",
    "# df_cus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_trx.shape)\n",
    "df_trx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_1=pd.DataFrame(df_trx.products.str.split('|').tolist(), index=df_trx.customerId)\\\n",
    ".stack()\\\n",
    ".reset_index()\\\n",
    ".groupby(['customerId', 0])\\\n",
    ".agg({0: 'count'})\\\n",
    ".rename(columns={0: 'purchase_count'})\\\n",
    ".reset_index()\\\n",
    ".rename(columns={0: 'productId'})\n",
    "\n",
    "# Store \n",
    "data_prep_1.to_csv(\"\".join([interdata, 'syntetic_purchase_count.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_prep_1.head())\n",
    "print('*'*40)\n",
    "print(data_prep_1.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's normalize data in a way to have a rank\n",
    "data_prep_2 = pd.pivot_table(data=data_prep_1, index='customerId', columns='productId', values='purchase_count', aggfunc='sum')\n",
    "data_prep_3 = (data_prep_2 - data_prep_2.min())/(data_prep_2.max() - data_prep_2.min())\n",
    "data_prep_4 = data_prep_3.reset_index().melt(id_vars=['customerId'], value_name='prod_ratings').dropna()\n",
    "data_prep_4.index = np.arange(0, len(data_prep_4))\n",
    "data_prep_4['prod_ratings'] = data_prep_4['prod_ratings'].apply(lambda x: int((round(x, 2))*10))\n",
    "\n",
    "#Store\n",
    "data_prep_4.to_csv(\"\".join([interdata, 'syntetic_prod_ratings.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_prep_4.sort_values(by=['customerId']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get **Triplet Representation** with each column representing user, item and the given rating respectively.\n",
    "Notice the rating goes from 0â€“100 (with 100 being the most number of purchase for an item and 0 being 0 purchase count for that item).\n",
    "It's a kind of preference for each user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Explore syntetic ratings by UserID and ProductID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt1=data_prep_4['prod_ratings']\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt1)\n",
    "ax.set_xlabel('Syntetic Product Ratings')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Rating Distribution of {} items'.format(data_prep_4.shape[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Distribution by ProductId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2=data_prep_4.groupby('productId')['prod_ratings'].count().reset_index()[0:50]\n",
    "df_plt_2\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.bar(df_plt_2.productId, df_plt_2.prod_ratings, )\n",
    "ax.set_xlabel('Number of Ratings per product')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by ProductId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2.sort_values(by='prod_ratings', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_2=data_prep_4.groupby('productId')['prod_ratings'].count()\n",
    "df_plt_2\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt_2)\n",
    "ax.set_xlim(0, 3000)\n",
    "ax.set_xlabel('Number of Ratings')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by ProductId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Distribution By User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3=data_prep_4.groupby('customerId')['prod_ratings'].count().reset_index()[0:50]\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.bar(df_plt_3.customerId, df_plt_3.prod_ratings)\n",
    "ax.set_xlabel('Number of Ratings per Customer')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Rating Distribution by customerId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3.sort_values(by='prod_ratings', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plt_3=data_prep_4.groupby('customerId')['prod_ratings'].count()\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "plt.hist(df_plt_3)\n",
    "ax.set_xlim(0, 50)\n",
    "ax.set_xlabel('Number of Ratings per Customer')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Rating Distribution by customerId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "To do that, I use **Surprise** which is an easy-to-use Python scikit for recommender systems.\n",
    "\n",
    "I follow the Getting Started (https://surprise.readthedocs.io/en/stable/getting_started.html#getting-started)\n",
    "\n",
    "Reference: https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and Utility Functions (mlflow_tracker, get_Iu, get_Ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order)\n",
    "data_prep_5 = data_prep_4.rename(columns={'customerId': 'userID', 'productId':'itemID', 'prod_ratings':'rating'})\n",
    "data_prep_5.to_csv(os.path.join(outdata, 'abt.csv'))\n",
    "\n",
    "#Create the experiment\n",
    "mlflow.set_experiment('Purchase Recommended System')\n",
    "\n",
    "#Create Minio Bucket for Artefact Storage\n",
    "%run -t 00_set_bucket.ipynb\n",
    "\n",
    "\n",
    "def mlflow_tracker(data, algo, params, rundesc='myrun'):\n",
    "    \"\"\" \n",
    "    return the run id and experiment id of each model\n",
    "    args: \n",
    "      reader_data: data parsed with Reader class \n",
    "      params: paramters of cross_validate function\n",
    "      rundesc: A description to populate exoeriment note\n",
    "      \n",
    "    returns: \n",
    "      run_id and experiment_id\n",
    "    \"\"\"\n",
    "        \n",
    "    # Store Algo name\n",
    "    algo_name = 'CrossValidation' + str(algo.__class__.__name__)\n",
    "\n",
    "    with mlflow.start_run(run_name=algo_name) as run:\n",
    "\n",
    "        # Store run_id and experiment_id\n",
    "        run_id=run.info.run_uuid\n",
    "        experiment_id=run.info.experiment_id\n",
    "        \n",
    "        #Read data\n",
    "        mindata = data.rating.min()\n",
    "        maxdata = data.rating.max()\n",
    "        reader = Reader(rating_scale=(mindata,maxdata))\n",
    "        data_parse = Dataset.load_from_df(data[['userID', 'itemID', 'rating']], reader)\n",
    "        \n",
    "        #Create model instance on a 3-fold cross validation\n",
    "        redic=cross_validate(algo, data_parse, **params)\n",
    "        #Create a dataframe of means\n",
    "        recdf=pd.DataFrame.from_dict(redic).mean(axis=0)\n",
    "\n",
    "        #Log params\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric('test_time', recdf.iloc[3])\n",
    "        mlflow.log_metric('test_rmse_mean', recdf.iloc[0])\n",
    "        mlflow.log_metric('test_mae_mean', recdf.iloc[1])\n",
    "        mlflow.log_metric('fit_time', recdf.iloc[2])     \n",
    "        \n",
    "        # Set the notes for Runs\n",
    "        MlflowClient().set_tag(run_id,\n",
    "                               \"mlflow.note.content\",\n",
    "                               rundesc)\n",
    "\n",
    "    return (run_id, experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage: Automatic cross-validation\n",
    "\n",
    "#### Basic Algorithms\n",
    "\n",
    "##### random_pred.NormalPredictor\n",
    "Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "##### baseline_only.BaselineOnly\n",
    "Algorithm predicting the baseline estimate for given user and item.\n",
    "\n",
    "#### KNN algorithms\n",
    "\n",
    "##### knns.KNNBasic\n",
    "A basic collaborative filtering algorithm\n",
    "##### knns.KNNWithMeans\n",
    "A basic collaborative filtering algorithm, taking into account the mean ratings of each user.\n",
    "##### knns.KNNWithZScore\n",
    "A basic collaborative filtering algorithm, taking into account taking into account the z-score normalization of each user.\n",
    "##### knns.KNNBaseline\n",
    "A basic collaborative filtering algorithm taking into account a baseline rating.\n",
    "\n",
    "#### Matrix Factorization-based algorithms\n",
    "##### matrix_factorization.SVD\n",
    "The famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize.When baselines are not used, this is equivalent to Probabilistic Matrix Factorization\n",
    "##### matrix_factorization.SVDpp\n",
    "The SVD++ algorithm, an extension of SVD taking into account implicit ratings.\n",
    "##### matrix_factorization.NMF\n",
    "A collaborative filtering algorithm based on Non-negative Matrix Factorization.\n",
    "\n",
    "#### Other Algorithms\n",
    "##### slope_one.SlopeOne\n",
    "A simple yet accurate collaborative filtering algorithm.\n",
    "##### co_clustering.CoClustering\n",
    "A collaborative filtering algorithm based on co-clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'measures': ['RMSE', 'MAE'], 'cv':3, 'verbose': False}\n",
    "algos = [BaselineOnly()]\n",
    "\n",
    "# , NormalPredictor(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), KNNBaseline(), SVD(), SVDpp(), SlopeOne(), CoClustering()\n",
    "\n",
    "for algo in algos:\n",
    "    mlflow_tracker(data_prep_5, algo, params)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:  Based on the Mlflow tracking service, the best model is BaselineOnly algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reader(df):\n",
    "    \"\"\"\n",
    "    return a data parsed with Reader class\n",
    "    args:\n",
    "        df: pandas dataframe\n",
    "    returns:\n",
    "        data parsed\n",
    "    \"\"\"\n",
    "    mindata = df.rating.min()\n",
    "    maxdata = df.rating.max()\n",
    "    reader = Reader(rating_scale=(mindata,maxdata))\n",
    "    data = Dataset.load_from_df(df[['userID', 'itemID', 'rating']], reader)\n",
    "    return data\n",
    "    \n",
    "def preparer(data, method=None, test_size=0.25):\n",
    "    \"\"\" \n",
    "    return train and test sets \n",
    "    args: \n",
    "      data: data parsed with Reader class\n",
    "      method: for sampling data. 'train_test_split' option. Default None\n",
    "    returns: \n",
    "      trainset and testset\n",
    "    \"\"\"\n",
    "    if method == 'train_test_split':\n",
    "        trainset, testset = train_test_split(data, test_size=test_size)\n",
    "    trainset = data.build_full_trainset()\n",
    "    testset = trainset.build_testset()\n",
    "    \n",
    "    return trainset, testset\n",
    "\n",
    "def trainer(trainset, bsl_options):\n",
    "    \"\"\" \n",
    "    return trained model \n",
    "    args: \n",
    "      trainset: training data parsed with Reader class\n",
    "      bsl_option: algorithm options \n",
    "    returns: \n",
    "      trained model\n",
    "    \"\"\"\n",
    "    algo = BaselineOnly(bsl_options=bsl_options)\n",
    "    model = algo.fit(trainset)\n",
    "    return model\n",
    "\n",
    "def predictor(model, testset):\n",
    "    \"\"\" \n",
    "    return trained model and predictions\n",
    "    args: \n",
    "      model: trained model\n",
    "      testset: test data parsed with Reader class \n",
    "    returns: \n",
    "      trained model and predictions\n",
    "    \"\"\"\n",
    "    predictions = model.test(testset)\n",
    "    return model, predictions\n",
    "\n",
    "def tuner(data, bsl_options_grid, param_model):\n",
    "    \"\"\" \n",
    "    return model parameters, tuning history and best tuned model\n",
    "    args: \n",
    "      data: data parsed with Reader class \n",
    "      bsl_options_grid:  algorithm options grid\n",
    "      param_model: error measures and cross validation parameters \n",
    "    returns: \n",
    "      param_model, history_tune, tuned_model\n",
    "    \"\"\"\n",
    "    \n",
    "    gs = GridSearchCV(BaselineOnly, bsl_options_grid, **param_model)\n",
    "    gs.fit(data)\n",
    "    \n",
    "    history_tune=pd.DataFrame.from_dict(gs.cv_results)\n",
    "    \n",
    "    best_bsl_options = gs.best_params['rmse']\n",
    "    \n",
    "    return param_model, history_tune\n",
    "\n",
    " # Define utils functions for prediction readability\n",
    "\n",
    "def get_Iu(trainset, uid):\n",
    "    \"\"\"\n",
    "    return the number of items rated by given user\n",
    "    args: \n",
    "      uid: the id of the user\n",
    "    returns: \n",
    "      the number of items rated by the user\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0\n",
    "\n",
    "def get_Ui(trainset, iid):\n",
    "    \"\"\"\n",
    "    return number of users that have rated given item\n",
    "    args:\n",
    "      iid: the raw id of the item\n",
    "    returns:\n",
    "      the number of users that have rated the item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "def plotter_hist(data, productId):\n",
    "    plt.hist(data_prep_4.loc[data_prep_4['productId'] == '2']['prod_ratings'])\n",
    "    plt.xlabel('rating')\n",
    "    plt.ylabel('Number of ratings')\n",
    "    plt.title('Number of ratings 2 has received')\n",
    "\n",
    "def mlflow_tune_tracker(data, algo_name, param_grid, param_model, method=None, rundesc='myruntuned'):\n",
    "    \"\"\" \n",
    "    return the run id and experiment id of tuned model\n",
    "    args: \n",
    "      algo_name: the name of tuned algorithm \n",
    "      param_grid:  algorithm options grid\n",
    "      param_model: error measures and cross validation parameters \n",
    "    returns: \n",
    "      run_id, experiment_id\n",
    "    \"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=algo_name) as run:\n",
    "\n",
    "        # Store run_id and experiment_id\n",
    "        run_id=run.info.run_uuid\n",
    "        experiment_id=run.info.experiment_id\n",
    "        \n",
    "        #Read data\n",
    "        data_parse = reader(data)\n",
    "\n",
    "        #Tune\n",
    "        params, history_tune = tuner(data_parse, param_grid, param_model)\n",
    "\n",
    "        #History\n",
    "        for index, row in history_tune.iterrows():\n",
    "            with mlflow.start_run(experiment_id=experiment_id, run_name=algo_name + str(index), nested=True) as subruns:\n",
    "\n",
    "                #Set variables \n",
    "                bsl_options = row['params']\n",
    "                params_tune = {**params, **bsl_options}\n",
    "                trainset, testset = preparer(data_parse, method)\n",
    "\n",
    "                #Log params\n",
    "                mlflow.log_params(params_tune)\n",
    "                mlflow.log_metric('fit_time',round(row['mean_fit_time'], 3))\n",
    "                mlflow.log_metric('test_time', round(row['mean_test_time'], 3))\n",
    "                mlflow.log_metric('test_rmse_mean', round(row['mean_test_rmse'], 3))\n",
    "                mlflow.log_metric('test_mae_mean', round(row['mean_test_mae'], 3))\n",
    "                \n",
    "                #Log Model (artefact)\n",
    "                temp = tempfile.NamedTemporaryFile(prefix=\"model_\", suffix=\".pkl\")\n",
    "                temp_name = temp.name\n",
    "                try:\n",
    "                    model, predictions = predictor(trainer(trainset, bsl_options), testset)\n",
    "                    dump.dump(temp_name, predictions, model)\n",
    "                    mlflow.log_artifact(temp_name, 'model')\n",
    "                finally:\n",
    "                    temp.close()\n",
    "\n",
    "                 #Log best and worst predictions. Log charts for validation\n",
    "                df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "                df['Iu'] = [get_Iu(trainset, uid) for uid in df.uid]\n",
    "                df['Ui'] = [get_Ui(trainset, iid) for iid in df.iid]\n",
    "                df['err'] = abs(df.est - df.rui)\n",
    "                best_predictions = df.sort_values(by='err')[:10]\n",
    "                worst_predictions = df.sort_values(by='err')[-10:]\n",
    "                \n",
    "                temp_dir = tempfile.TemporaryDirectory(dir  =  outdata, prefix='predictions_')\n",
    "                temp_dirname = temp_dir.name\n",
    "                temp_file_best = tempfile.NamedTemporaryFile(prefix=\"best-predicitions_\", suffix=\".csv\", dir=temp_dirname)\n",
    "                temp_filename_best = temp_file_best.name\n",
    "                temp_file_worst = tempfile.NamedTemporaryFile(prefix=\"worst-predicitions_\", suffix=\".csv\", dir=temp_dirname)\n",
    "                temp_filename_worst = temp_file_worst.name\n",
    "                try:\n",
    "                    best_predictions.to_csv(temp_filename_best, index=False)                    \n",
    "                    worst_predictions.to_csv(temp_filename_worst, index=False)\n",
    "                    mlflow.log_artifact(temp_dirname)\n",
    "                finally:\n",
    "                    temp_file_best.close()\n",
    "                    temp_file_worst.close()\n",
    "                    temp_dir.cleanup()\n",
    "                \n",
    "                    \n",
    "        MlflowClient().set_tag(run_id,\n",
    "                   \"mlflow.note.content\",\n",
    "                   rundesc)\n",
    "\n",
    "#     return run_id, experiment_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'bsl_options' : {'method': ['als', 'sgd'],\n",
    "              'n_epochs': [5, 15],\n",
    "              'reg_u': [10, 20],\n",
    "              'reg_i': [5,15]               \n",
    "                }\n",
    "              }\n",
    "\n",
    "param_model = {\n",
    "          'measures': ['RMSE', 'MAE'], \n",
    "          'cv':3\n",
    "            }\n",
    "\n",
    "# run_id, experiment_id = mlflow_tune_tracker(data_prep_5, 'BaselineOnly', param_grid, param_model)\n",
    "mlflow_tune_tracker(data_prep_5, 'BaselineOnly', param_grid, param_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "Comment the best and the worst predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best parameter combination**\n",
    "\n",
    "Then the best model is BaselineOnly6 with \n",
    "- method: 'als'\n",
    "- n_epochs: 15\n",
    "- reg_u: 20\n",
    "- reg_i: 5\n",
    "\n",
    "**Compare to all the experiments we ran so we validate the model as the best one based on RMSE metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from MLflow server the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_name = 'BaselineOnly4'\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name('Purchase Recommended System')\n",
    "experiment_id = experiment.experiment_id\n",
    "#Store run info in a dataframe\n",
    "\n",
    "run = MlflowClient().search_runs(\n",
    "    experiment_ids=experiment_id,\n",
    "    filter_string=\"tags.mlflow.runName = '{}'\".format(champion_name)\n",
    ")\n",
    "\n",
    "run_id = run[0].info.run_id\n",
    "artifacts_list = [arts.path for arts in client.list_artifacts(run_id, path=None)]\n",
    "\n",
    "for art_path in artifacts_list: \n",
    "    client.download_artifacts(run_id, art_path, outmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Best and Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions = pd.read_csv(\"/\".join([outmodels, 'predictions_5qrt6nur/best-predicitions_218fjq_u.csv']))\n",
    "worst_predictions = pd.read_csv(\"/\".join([outmodels, 'predictions_5qrt6nur/worst-predicitions_kititvv8.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Because Ui is anywhere between 275 to 2792, it's good because we have a lot of people who buy the product (a kind of preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(data_prep_4.loc[data_prep_4['productId'] == str(best_predictions.iloc[0]['iid'])]['prod_ratings'])\n",
    "\n",
    "ax.set_ylabel('Number of ratings')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_title('Number of ratings 2 has received')\n",
    "plt.show()\n",
    "\n",
    "# # save the figure\n",
    "fig.savefig(outreports + 'N_Ratings_2.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**: Really bad. Because consider the item 148, the user buy it 0 times but the algorithm says he buy always, Also if we look at distribution only few people buy it frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(data_prep_4.loc[data_prep_4['productId'] == str(best_predictions.iloc[9]['iid'])]['prod_ratings'], color='red')\n",
    "ax.set_xlabel('Rating')\n",
    "ax.set_ylabel('Number of ratings')\n",
    "ax.set_title('Number of ratings 211 has received')\n",
    "plt.show();\n",
    "# # save the figure\n",
    "fig.savefig(outreports + 'N_Ratings_211.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.log_artifact(run_id,outreports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registration\n",
    "\n",
    "Register the Model in the Mlflow Model Registry and set its status as STAGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Champion'\n",
    "artifact_path = \"model\"\n",
    "model_uri = \"runs:/{run_id}/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "\n",
    "model_details = mlflow.register_model(model_uri=model_uri, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "client.update_registered_model(\n",
    "  name=model_details.name,\n",
    "  description=\"This model provides recommendation for specific users and items based on purchase data. The data consists of user transactions\"\n",
    ")\n",
    "\n",
    "client.update_model_version(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  description=\"This model was built with Surprise library. It is a ALS based BaselineOnly algorithm\"\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    "  stage='Staging',\n",
    ")\n",
    "model_version_details = client.get_model_version(\n",
    "  name=model_details.name,\n",
    "  version=model_details.version,\n",
    ")\n",
    "print(\"The current model stage is: '{stage}'\".format(stage=model_version_details.current_stage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
