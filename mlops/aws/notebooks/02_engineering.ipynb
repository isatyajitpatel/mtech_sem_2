{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Recommendation System for Purchase Data\n",
    "\n",
    "The scope of this notebook is \n",
    "\n",
    "- Prepare the Scoring Function\n",
    "- Unit Test the Score\n",
    "- Build Flask Scoring App and deploy as a Web End point\n",
    "- Build Dash App and deploy as Interactive Web Services\n",
    "\n",
    "The business case is an in-store app allowing its customers to place orders before they even have to walk into the store.\n",
    "When a customer provides its own id and the app suggests\n",
    "\n",
    "- Personalized recommendation with ranked list of items (product IDs) that the user is most likely to want to put in his/her (empty) “basket”\n",
    "\n",
    "Assuming that the scenario is ModelOps 0. \n",
    "\n",
    "Then: \n",
    "\n",
    "1. Data scientists hand over a trained model as an artifact to the engineering team for deployement\n",
    "2. The handoff can include putting the trained model in the models registry\n",
    "3. The Scoring process is in Batch on a sigle EC2 instance\n",
    "\n",
    "We have to reproduce the required development enviroment\n",
    "\n",
    "0. We get the last version of Champion Model (optional)\n",
    "\n",
    "1. Define Scoring Functions: Batch scoring is the main assumption\n",
    "\n",
    "    - Define the get_top_items function \n",
    "    - Define the get_top_n_ui function\n",
    "    \n",
    "2. Unit Test \n",
    "\n",
    "3. Deploy Model as Scoring Web EndPoint\n",
    "\n",
    "4. Deploy Model as Interactive Web Service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import sqlalchemy as sql\n",
    "\n",
    "#Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import surprise\n",
    "from surprise import dump\n",
    "\n",
    "#Model Tracking\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "#ML engineering\n",
    "import flask\n",
    "from flask import Flask, jsonify, Response\n",
    "import gunicorn\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import configparser\n",
    "import json\n",
    "import pickle\n",
    "import unittest\n",
    "import docker\n",
    "import pprint\n",
    "import time\n",
    "import requests\n",
    "\n",
    "#Settings\n",
    "from pprint import pprint\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set enviroment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviroment variables\n",
    "outmodels = '../models/'\n",
    "app_folder = '../src/score_endpoint'\n",
    "dash_folder = '../score_interactive_endpoint'\n",
    "docker_folder = './docker/'\n",
    "\n",
    "# Set dbconnection variables\n",
    "dbconnPath = './dbconn.properties'\n",
    "config = configparser.RawConfigParser()\n",
    "config.read(dbconnPath)\n",
    "params = config\n",
    "db_host=params.get('CONN', 'host')\n",
    "db_port=params.get('CONN', 'port')\n",
    "db_user=params.get('CONN', 'user')\n",
    "db_pwd=params.get('CONN', 'password')\n",
    "db_name=params.get('CONN', 'database')\n",
    "\n",
    "# Set connection string\n",
    "connection_str = f'mysql+pymysql://{db_user}:{db_pwd}@{db_host}:{db_port}/{db_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Artefact from Mlflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "for regmodel in client.list_registered_models():\n",
    "    regmodel_info = dict(regmodel)\n",
    "\n",
    "# pprint(regmodel_info, indent=3)\n",
    "\n",
    "champion=client.get_registered_model('Champion')\n",
    "championid=champion.latest_versions[-1].run_id\n",
    "\n",
    "art_list = [arts.path for arts in client.list_artifacts(championid, path=None)]\n",
    "\n",
    "for art_path in art_list: \n",
    "    client.download_artifacts(championid, art_path, outmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the Model Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpkl = [modelpath for modelpath in glob.glob(outmodels + 'model/*.pkl')][0]\n",
    "modelpkl\n",
    "\n",
    "predictions, algo = dump.load(modelpkl)\n",
    "\n",
    "print('\\n')\n",
    "print('Sample of Predictions: ')\n",
    "print('\\n', predictions[0:10])\n",
    "print('\\n', 'Number of predictions:', len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Scoring Function\n",
    "\n",
    "We have to return Top 10 Recommended Items by userid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(predictions, n=10):\n",
    "    \n",
    "    '''\n",
    "    Returns the the top-N recommendation from a set of predictions\n",
    "    \n",
    "    '''\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "        \n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "        \n",
    "    return top_n\n",
    "\n",
    "def get_top_n_ui(top, uid):\n",
    "    try:\n",
    "        top_n_ui = [[iid for (iid, _) in user_ratings] for UID, user_ratings in top.items() if UID==uid][0]\n",
    "        return top_n_ui\n",
    "    except ValueError: # user was not part of the trainset\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestScoreFunction(unittest.TestCase):\n",
    "    \n",
    "#     def setUp(self):\n",
    "#         self.testcase = \"100\"\n",
    "#         self.expected = ['6', '118', '67', '27', '0', '51', '282', '62', '24', '201']\n",
    "    \n",
    "#     def test_empty(self):\n",
    "#         self.assertTrue(bool(get_top_n_ui(get_top(predictions), self.testcase)))\n",
    "\n",
    "#     def test_basic(self):\n",
    "#         self.assertEqual(get_top_n_ui(get_top(predictions), self.testcase), self.expected)\n",
    "        \n",
    "# unittest.main(argv = ['first-arg-is-ignored'], exit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment: Scoring Web EndPoint and WSGI server \n",
    "\n",
    "Because we're testing, I need a app folder with the model and: \n",
    "\n",
    "1. app.py\n",
    "2. wsgi.py\n",
    "3. requirements.txt\n",
    "3. Dockerfile\n",
    "\n",
    "Then I will run the application with Docker Client and I will test it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a app folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(app_folder):\n",
    "    os.makedirs(app_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(app_folder + '/model'):\n",
    "    shutil.copytree(src=outmodels + 'model', dst=app_folder + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(app_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create app.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "A basic Flask application for scoring web endpoint.\n",
    "'''\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from surprise import dump\n",
    "\n",
    "import flask\n",
    "from flask import Flask, jsonify, Response\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#create an instance\n",
    "app = Flask(__name__)\n",
    "\n",
    "def locate_model(dest):\n",
    "    \n",
    "    \"\"\" \n",
    "    return path of binary model\n",
    "    args:\n",
    "       dest: folder for searching\n",
    "    returns:\n",
    "       model_path\n",
    "    \"\"\"\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(dest):\n",
    "        for filename in [f for f in filenames if f.endswith((\".pkl\", \".pickle\"))]:\n",
    "            model_path = os.path.join(dirpath, filename)\n",
    "            return model_path\n",
    "    return None\n",
    "\n",
    "def model_reader(model_path):\n",
    "    \"\"\" \n",
    "    return predictions and model class\n",
    "    args:\n",
    "       model_path: pickle file path\n",
    "    returns:\n",
    "       predictions and model\n",
    "    \"\"\"\n",
    "    predictions, algo = dump.load(model_path)\n",
    "    return predictions, algo\n",
    "\n",
    "def get_top(predictions, n=10):\n",
    "    \n",
    "    '''\n",
    "    Returns the the top-N recommendation from a set of predictions\n",
    "    args:\n",
    "       predictions: predictions generated in testing phase\n",
    "       n: number of items to suggest (default=10)\n",
    "    return:\n",
    "       top_n dictionary: user-prediction dictionaries\n",
    "    '''\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "        \n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "        \n",
    "    return top_n\n",
    "\n",
    "def get_top_n_ui(top, uid):\n",
    "    '''\n",
    "    Returns the list of selected items\n",
    "    args:\n",
    "       top: user-prediction dictionaries\n",
    "       uid: user id for filtering\n",
    "    return:\n",
    "       top_n dictionary: user-prediction dictionaries\n",
    "    '''\n",
    "    try:\n",
    "        top_n_ui = [[iid for (iid, _) in user_ratings] for UID, user_ratings in top.items() if UID==uid][0]\n",
    "        return top_n_ui\n",
    "    except ValueError: \n",
    "        return 0\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def health_checker():\n",
    "    text = 'The Scoring application is ready!'\n",
    "    return Response(text, status=200, mimetype='text/plain')\n",
    "\n",
    "@app.route('/predict', methods=['GET','POST'])\n",
    "def predictor():\n",
    "\n",
    "    #Intiate variables\n",
    "    data = defaultdict()\n",
    "    data[\"success\"] = False\n",
    "    params = flask.request.args\n",
    "    \n",
    "    if 'uid' in params.keys():\n",
    "        uid_toscore = str(params.get('uid'))\n",
    "        model_path = locate_model(os.getcwd())\n",
    "        predictions, _ = model_reader(model_path)\n",
    "        uid_predictions = get_top_n_ui(get_top(predictions), uid_toscore)\n",
    "        \n",
    "        prediction_rank_lenght = len(uid_predictions)\n",
    "        prediction_rank_labels = [\"\".join([\" Product\", str(i)]) for i in range(1,prediction_rank_lenght)]\n",
    "        products_recommended = pd.DataFrame(list(zip(prediction_rank_labels, uid_predictions)), columns=['Product_Rank', 'Product_id'])\n",
    "\n",
    "        data['response'] = products_recommended.to_dict()\n",
    "        data['success'] = True\n",
    "    \n",
    "    return flask.jsonify(data)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=9999, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flask.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(surprise.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "numpy\n",
    "pandas\n",
    "scikit-surprise\n",
    "flask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create dockerfile for app enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Use an official Python runtime as parent image\n",
    "FROM python:3\n",
    "    \n",
    "LABEL Scoring App = \"Recommendation System - Python - Surprise\"\n",
    "\n",
    "# RUN apt-get update && apt-get install -y \\\n",
    "# python3-dev \\\n",
    "# build-essential    \n",
    "        \n",
    "# Set the working directory to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "ADD . /app\n",
    "\n",
    "# Upgrade pip and install any needed packages specified in requirements.txt\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 9999 available\n",
    "EXPOSE 9999\n",
    "\n",
    "#Entrypoint python exec\n",
    "ENTRYPOINT [ \"python\" ]\n",
    "\n",
    "#Run flask application\n",
    "CMD [ \"app.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Application with Python Docker Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"score-flask_app:1\"\n",
    "\n",
    "#Client instance\n",
    "dockercli = docker.DockerClient()\n",
    "\n",
    "#Check for image\n",
    "if not dockercli.images.list(image_name):\n",
    "    #if not build it\n",
    "    dockercli.images.build(path='.', tag = image_name)\n",
    "else:\n",
    "    dockercli.images.remove(image_name, force = True)\n",
    "    dockercli.images.build(path='.', tag = image_name)\n",
    "try:\n",
    "    app_container = dockercli.containers.run(image_name, name='scoring_app_test', detach=True, ports={'9999/tcp': 9999})\n",
    "    status = app_container.attrs\n",
    "    print(status['State'])\n",
    "    while (status['State']['Running'] == False):\n",
    "        time.sleep(3)\n",
    "        app_container.reload()\n",
    "        status = app_container.attrs\n",
    "        print(''.center(50, '-'))\n",
    "        print(status['State'])\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Flask Scoring App as web endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = 'http'\n",
    "server = '10.249.21.252'\n",
    "port = '9999'\n",
    "\n",
    "# Check that the container is available\n",
    "check_request = requests.get(protocol + \"://\" + server + \":\" + port + \"/\")\n",
    "check_response = check_request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'uid': 100}\n",
    "\n",
    "# Scoring new data\n",
    "score_request = requests.get(protocol + \"://\" + server + \":\" + port + \"/predict\", params=params)\n",
    "score_response = score_request.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kill the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop and remove the container\n",
    "app_container.stop()\n",
    "app_container.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment: A basic Scoring Interactive Web Service\n",
    "\n",
    "Because our scenario is going to have a on-site application, I've just prototype a basic Scoring Interactive Web Service.\n",
    "Again I need an app folder with model and:\n",
    "\n",
    "1. app.py\n",
    "2. layout.py\n",
    "3. callbacks.py\n",
    "4. run.py\n",
    "\n",
    "And again i'll run the app and I'll test it with Python Docker client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the app folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dash_folder):\n",
    "    os.makedirs(dash_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dash_folder + '/model'):\n",
    "    shutil.copytree(src='../' + outmodels + 'model', dst=dash_folder + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dash_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create app.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "'''\n",
    "The Dash instance module\n",
    "'''\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from surprise import dump\n",
    "\n",
    "import dash\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets, suppress_callback_exceptions = True)\n",
    "server = app.server\n",
    "\n",
    "def locate_model(dest):\n",
    "    \n",
    "    \"\"\" \n",
    "    return path of binary model\n",
    "    args:\n",
    "       dest: folder for searching\n",
    "    returns:\n",
    "       model_path\n",
    "    \"\"\"\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(dest):\n",
    "        for filename in [f for f in filenames if f.endswith((\".pkl\", \".pickle\"))]:\n",
    "            model_path = os.path.join(dirpath, filename)\n",
    "            return model_path\n",
    "    return None\n",
    "\n",
    "def model_reader(model_path):\n",
    "    \"\"\" \n",
    "    return predictions and model class\n",
    "    args:\n",
    "       model_path: pickle file path\n",
    "    returns:\n",
    "       predictions and model\n",
    "    \"\"\"\n",
    "    predictions, algo = dump.load(model_path)\n",
    "    return predictions, algo\n",
    "\n",
    "def get_top(predictions, n=10):\n",
    "    \n",
    "    '''\n",
    "    Returns the the top-N recommendation from a set of predictions\n",
    "    args:\n",
    "       predictions: predictions generated in testing phase\n",
    "       n: number of items to suggest (default=10)\n",
    "    return:\n",
    "       top_n dictionary: user-prediction dictionaries\n",
    "    '''\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "        \n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "        \n",
    "    return top_n\n",
    "\n",
    "def get_top_n_ui(top, uid):\n",
    "    '''\n",
    "    Returns the list of selected items\n",
    "    args:\n",
    "       top: user-prediction dictionaries\n",
    "       uid: user id for filtering\n",
    "    return:\n",
    "       top_n dictionary: user-prediction dictionaries\n",
    "    '''\n",
    "    try:\n",
    "        top_n_ui = [[iid for (iid, _) in user_ratings] for UID, user_ratings in top.items() if UID==uid][0]\n",
    "        return top_n_ui\n",
    "    except ValueError: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create layouts.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile layouts.py\n",
    "\n",
    "'''\n",
    "This module contains the layout of the application\n",
    "'''\n",
    "\n",
    "import dash_table\n",
    "import dash_core_components as component\n",
    "import dash_html_components as html\n",
    "\n",
    "#Customerid Input\n",
    "c_id_component = component.Input(id=\"uid\", \n",
    "                                 type=\"text\", \n",
    "                                 placeholder='100')\n",
    "\n",
    "c_id_div = html.Div(id='customerid', children=[html.H6(\"Please enter your customer ID : \"), \n",
    "                                               c_id_component,\n",
    "                                               html.Br()], \n",
    "                   className=\"two columns\")\n",
    "\n",
    "#Prediction Output\n",
    "predictions = dash_table.DataTable(id='table',\n",
    "                                   css=[{'selector': 'table', 'rule': 'table-layout: fixed'}],\n",
    "                                   style_as_list_view=True, \n",
    "                                   style_cell={'width': '30%', 'textAlign': 'left'},\n",
    "                                   style_header={'backgroundColor': 'white', 'fontWeight': 'bold'})\n",
    "\n",
    "predictions_div = html.Div(id='predictions', \n",
    "                           children=[html.H6(children='Products Recommended'),\n",
    "                                     predictions],\n",
    "                           style={'width': '25%'}, \n",
    "                           className=\"ten columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the callbacks.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile callbacks.py\n",
    "\n",
    "'''\n",
    "This module contains the predict callback \n",
    "\n",
    "'''\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "from app import *\n",
    "\n",
    "@app.callback([Output('table', component_property='columns'), Output('table', component_property='data')],[Input(component_id='uid', component_property='value')])\n",
    "def predict(uid):\n",
    "    columns = []\n",
    "    products_recommended = []\n",
    "    if uid:\n",
    "        model_path = locate_model(os.getcwd())\n",
    "        predictions, _ = model_reader(model_path)\n",
    "        uid_predictions = get_top_n_ui(get_top(predictions), uid)\n",
    "        prediction_rank_lenght = len(uid_predictions)\n",
    "        prediction_rank_labels = [\"\".join([\" Product\", str(i)]) for i in range(1,prediction_rank_lenght)]\n",
    "        products_recommended = pd.DataFrame(list(zip(prediction_rank_labels, uid_predictions)), columns=['Product_Rank', 'Product_id'])\n",
    "        columns=[{\"name\": i, \"id\": i} for i in products_recommended.columns]\n",
    "        return columns, products_recommended.to_dict('records')\n",
    "    else:\n",
    "        return columns, products_recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the run.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.py\n",
    "\n",
    "'''\n",
    "This module runs the entire Dash Application\n",
    "\n",
    "'''\n",
    "\n",
    "import dash_table\n",
    "import dash_core_components as component\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "from app import *\n",
    "from layouts import c_id_div, predictions_div\n",
    "import callbacks\n",
    "\n",
    "\n",
    "app.layout= html.Div(\n",
    "    \n",
    "    id=\"score_gui\", children=[\n",
    "        html.H1('Recommendation System for Purchase Data'),\n",
    "        html.H2('Scoring Interactive Web Service (Basic Concept)'),\n",
    "        html.Br(), \n",
    "        html.Div(children=[c_id_div, predictions_div], className=\"row\")\n",
    "    ])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(host=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write the requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "\n",
    "numpy\n",
    "pandas\n",
    "scikit-surprise\n",
    "dash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM python:3.8\n",
    "    \n",
    "WORKDIR /app\n",
    "\n",
    "ADD . /app\n",
    "\n",
    "RUN pip install -r ./requirements.txt\n",
    "\n",
    "EXPOSE 8050\n",
    "\n",
    "ENTRYPOINT [ \"python\" ]\n",
    "\n",
    "CMD [ \"run.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the Application with Python Docker client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"score-dash_app:1\"\n",
    "\n",
    "#Client instance\n",
    "dockercli = docker.DockerClient()\n",
    "\n",
    "#Check for image\n",
    "if not dockercli.images.list(image_name):\n",
    "    #if not build it\n",
    "    dockercli.images.build(path='.', tag = image_name)\n",
    "else:\n",
    "    dockercli.images.remove(image_name, force = True)\n",
    "    dockercli.images.build(path='.', tag = image_name)\n",
    "try:\n",
    "    app_container = dockercli.containers.run(image_name, name='scoring_dash_app_test', detach=True, ports={'8050/tcp': 8050})\n",
    "    status = app_container.attrs\n",
    "    print(status['State'])\n",
    "    while (status['State']['Running'] == False):\n",
    "        time.sleep(3)\n",
    "        app_container.reload()\n",
    "        status = app_container.attrs\n",
    "        print(''.center(50, '-'))\n",
    "        print(status['State'])\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kill the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop and remove the container\n",
    "app_container.stop()\n",
    "app_container.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
